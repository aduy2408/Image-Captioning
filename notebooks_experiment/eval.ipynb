{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e623ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycocoevalcap in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (1.2)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from pycocoevalcap) (2.0.8)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.10.0)\n",
      "Requirement already satisfied: numpy in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from pycocotools>=2.0.2->pycocoevalcap) (2.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install\n",
    "!pip install pycocoevalcap\n",
    "\n",
    "# Simple usage\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "from pycocotools.coco import COCO\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdcba679",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_with_pycocoevalcap(eval_dir=\"eval_result_json\"):\n",
    "        \n",
    "    all_annotations = []\n",
    "    all_results = {}  \n",
    "    annotation_id = 0\n",
    "    image_id = 0\n",
    "    image_mapping = {} \n",
    "    \n",
    "    for filename in os.listdir(eval_dir):\n",
    "        if filename.endswith('.json'):\n",
    "            model_name = filename.replace('local_evaluation_', '').replace('.json', '')\n",
    "            \n",
    "            with open(os.path.join(eval_dir, filename), 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            model_results = []\n",
    "            \n",
    "            for image_key, image_data in data.items():\n",
    "                image_name = image_data['image_name']\n",
    "                \n",
    "                # Create consistent image_id mapping\n",
    "                if image_name not in image_mapping:\n",
    "                    image_mapping[image_name] = image_id\n",
    "                    \n",
    "                    # Add ground truth annotations \n",
    "                    for gt_caption in image_data['ground_truth']:\n",
    "                        all_annotations.append({\n",
    "                            \"image_id\": image_id,\n",
    "                            \"id\": annotation_id,\n",
    "                            \"caption\": gt_caption.strip()\n",
    "                        })\n",
    "                        annotation_id += 1\n",
    "                    \n",
    "                    image_id += 1\n",
    "                \n",
    "                # Add model prediction\n",
    "                current_image_id = image_mapping[image_name]\n",
    "                generated_caption = list(image_data['model_results'].values())[0]\n",
    "                \n",
    "                if \"failed\" not in generated_caption.lower() and \"error\" not in generated_caption.lower():\n",
    "                    model_results.append({\n",
    "                        \"image_id\": current_image_id,\n",
    "                        \"caption\": generated_caption.strip()\n",
    "                    })\n",
    "            \n",
    "            all_results[model_name] = model_results\n",
    "    \n",
    "    \n",
    "    # Create COCO ground truth object\n",
    "    coco_gt_data = {\n",
    "        'annotations': all_annotations,\n",
    "        'images': [{'id': img_id} for img_id in range(len(image_mapping))],\n",
    "        'info': {'description': 'Image Captioning Evaluation'},\n",
    "        'licenses': [],\n",
    "        'type': 'captions'\n",
    "    }\n",
    "    \n",
    "    # Initialize COCO ground truth\n",
    "    coco_gt = COCO()\n",
    "    coco_gt.dataset = coco_gt_data\n",
    "    coco_gt.createIndex()\n",
    "        \n",
    "    # Evaluate each model\n",
    "    evaluation_results = {}\n",
    "    \n",
    "    for model_name, model_results in all_results.items():\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as temp_file:\n",
    "            json.dump(model_results, temp_file)\n",
    "            temp_filename = temp_file.name\n",
    "        \n",
    "\n",
    "        coco_res = coco_gt.loadRes(temp_filename)\n",
    "        \n",
    "        coco_eval = COCOEvalCap(coco_gt, coco_res)\n",
    "        \n",
    "        coco_eval.evaluate()\n",
    "        \n",
    "        evaluation_results[model_name] = coco_eval.eval.copy()\n",
    "        \n",
    "        print(f\"✅ {model_name} evaluation complete\")\n",
    "            \n",
    " \n",
    "    return evaluation_results, coco_gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4218b99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_pycocoevalcap_results(evaluation_results):    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVALUATION RESULTS USING COCO\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    \n",
    "    table_data = []\n",
    "    for model_name, scores in evaluation_results.items():\n",
    "        if scores:  # Only include models with results\n",
    "            row = {\n",
    "                'Model': model_name.upper(),\n",
    "                'BLEU-1': f\"{scores.get('Bleu_1', 0):.4f}\",\n",
    "                'BLEU-2': f\"{scores.get('Bleu_2', 0):.4f}\",\n",
    "                'BLEU-3': f\"{scores.get('Bleu_3', 0):.4f}\",\n",
    "                'BLEU-4': f\"{scores.get('Bleu_4', 0):.4f}\",\n",
    "                'METEOR': f\"{scores.get('METEOR', 0):.4f}\",\n",
    "                'ROUGE-L': f\"{scores.get('ROUGE_L', 0):.4f}\",\n",
    "                'CIDEr': f\"{scores.get('CIDEr', 0):.4f}\",\n",
    "                'SPICE': f\"{scores.get('SPICE', 0):.4f}\"\n",
    "            }\n",
    "            table_data.append(row)\n",
    "        \n",
    "    df = pd.DataFrame(table_data)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6df1b298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 176, 'reflen': 131, 'guess': [176, 166, 156, 146], 'correct': [110, 57, 24, 8]}\n",
      "ratio: 1.3435114503714236\n",
      "Bleu_1: 0.625\n",
      "Bleu_2: 0.463\n",
      "Bleu_3: 0.321\n",
      "Bleu_4: 0.206\n",
      "computing METEOR score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 606 tokens at 21907.05 tokens per second.\n",
      "PTBTokenizer tokenized 200 tokens at 8423.68 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR: 0.297\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.475\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.540\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 1.127 s\n",
      "SPICE: 0.228\n",
      "✅ git evaluation complete\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 606 tokens at 16777.06 tokens per second.\n",
      "PTBTokenizer tokenized 125 tokens at 4262.32 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 114, 'reflen': 108, 'guess': [114, 104, 94, 84], 'correct': [57, 21, 2, 0]}\n",
      "ratio: 1.0555555555457818\n",
      "Bleu_1: 0.500\n",
      "Bleu_2: 0.318\n",
      "Bleu_3: 0.129\n",
      "Bleu_4: 0.000\n",
      "computing METEOR score...\n",
      "METEOR: 0.145\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.364\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.377\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 785.1 ms\n",
      "SPICE: 0.100\n",
      "✅ blip2 evaluation complete\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 606 tokens at 21824.99 tokens per second.\n",
      "PTBTokenizer tokenized 282 tokens at 8921.69 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 257, 'reflen': 130, 'guess': [257, 247, 237, 227], 'correct': [91, 18, 3, 1]}\n",
      "ratio: 1.97692307690787\n",
      "Bleu_1: 0.354\n",
      "Bleu_2: 0.161\n",
      "Bleu_3: 0.069\n",
      "Bleu_4: 0.035\n",
      "computing METEOR score...\n",
      "METEOR: 0.188\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.313\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.121\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 666.0 ms\n",
      "SPICE: 0.119\n",
      "✅ vit_gpt2 evaluation complete\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 606 tokens at 18801.09 tokens per second.\n",
      "PTBTokenizer tokenized 249 tokens at 9169.46 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 236, 'reflen': 131, 'guess': [236, 226, 216, 206], 'correct': [126, 59, 24, 9]}\n",
      "ratio: 1.8015267175435\n",
      "Bleu_1: 0.534\n",
      "Bleu_2: 0.373\n",
      "Bleu_3: 0.249\n",
      "Bleu_4: 0.161\n",
      "computing METEOR score...\n",
      "METEOR: 0.283\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.401\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.152\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 630.6 ms\n",
      "SPICE: 0.232\n",
      "✅ blip evaluation complete\n",
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS USING COCO\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>BLEU-1</th>\n",
       "      <th>BLEU-2</th>\n",
       "      <th>BLEU-3</th>\n",
       "      <th>BLEU-4</th>\n",
       "      <th>METEOR</th>\n",
       "      <th>ROUGE-L</th>\n",
       "      <th>CIDEr</th>\n",
       "      <th>SPICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GIT</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.4633</td>\n",
       "      <td>0.3208</td>\n",
       "      <td>0.2062</td>\n",
       "      <td>0.2972</td>\n",
       "      <td>0.4749</td>\n",
       "      <td>0.5401</td>\n",
       "      <td>0.2283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BLIP2</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.3177</td>\n",
       "      <td>0.1290</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>0.3642</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.0999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VIT_GPT2</td>\n",
       "      <td>0.3541</td>\n",
       "      <td>0.1606</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.1884</td>\n",
       "      <td>0.3127</td>\n",
       "      <td>0.1211</td>\n",
       "      <td>0.1192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BLIP</td>\n",
       "      <td>0.5339</td>\n",
       "      <td>0.3733</td>\n",
       "      <td>0.2493</td>\n",
       "      <td>0.1613</td>\n",
       "      <td>0.2827</td>\n",
       "      <td>0.4012</td>\n",
       "      <td>0.1519</td>\n",
       "      <td>0.2324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  BLEU-1  BLEU-2  BLEU-3  BLEU-4  METEOR ROUGE-L   CIDEr   SPICE\n",
       "0       GIT  0.6250  0.4633  0.3208  0.2062  0.2972  0.4749  0.5401  0.2283\n",
       "1     BLIP2  0.5000  0.3177  0.1290  0.0000  0.1454  0.3642  0.3771  0.0999\n",
       "2  VIT_GPT2  0.3541  0.1606  0.0689  0.0346  0.1884  0.3127  0.1211  0.1192\n",
       "3      BLIP  0.5339  0.3733  0.2493  0.1613  0.2827  0.4012  0.1519  0.2324"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, coco_gt = evaluate_with_pycocoevalcap(\"/home/duyle/Documents/Case-Study3/eval_result_json\")\n",
    "\n",
    "summary_df = display_pycocoevalcap_results(results)\n",
    "\n",
    "\n",
    "summary_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b73128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
