{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e623ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycocoevalcap in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (1.2)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from pycocoevalcap) (2.0.8)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.10.0)\n",
      "Requirement already satisfied: numpy in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from pycocotools>=2.0.2->pycocoevalcap) (2.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/duyle/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install\n",
    "!pip install pycocoevalcap\n",
    "\n",
    "# Simple usage\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "from pycocotools.coco import COCO\n",
    "import json\n",
    "import os\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcba679",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_with_pycocoevalcap(eval_dir=\"eval_json\"):\n",
    "    \"\"\"\n",
    "    Complete example of using pycocoevalcap with your evaluation files\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üöÄ Loading evaluation data...\")\n",
    "    \n",
    "    # Collect all data from your JSON files\n",
    "    all_annotations = []\n",
    "    all_results = {}  # Store results per model\n",
    "    \n",
    "    annotation_id = 0\n",
    "    image_id = 0\n",
    "    image_mapping = {}  # Map your image names to IDs\n",
    "    \n",
    "    # Process each model's evaluation file\n",
    "    for filename in os.listdir(eval_dir):\n",
    "        if filename.endswith('.json'):\n",
    "            model_name = filename.replace('local_evaluation_', '').replace('.json', '')\n",
    "            \n",
    "            with open(os.path.join(eval_dir, filename), 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            model_results = []\n",
    "            \n",
    "            for image_key, image_data in data.items():\n",
    "                image_name = image_data['image_name']\n",
    "                \n",
    "                # Create consistent image_id mapping\n",
    "                if image_name not in image_mapping:\n",
    "                    image_mapping[image_name] = image_id\n",
    "                    \n",
    "                    # Add ground truth annotations (only once per image)\n",
    "                    for gt_caption in image_data['ground_truth']:\n",
    "                        all_annotations.append({\n",
    "                            \"image_id\": image_id,\n",
    "                            \"id\": annotation_id,\n",
    "                            \"caption\": gt_caption.strip()\n",
    "                        })\n",
    "                        annotation_id += 1\n",
    "                    \n",
    "                    image_id += 1\n",
    "                \n",
    "                # Add model prediction\n",
    "                current_image_id = image_mapping[image_name]\n",
    "                generated_caption = list(image_data['model_results'].values())[0]\n",
    "                \n",
    "                # Skip failed predictions\n",
    "                if \"failed\" not in generated_caption.lower() and \"error\" not in generated_caption.lower():\n",
    "                    model_results.append({\n",
    "                        \"image_id\": current_image_id,\n",
    "                        \"caption\": generated_caption.strip()\n",
    "                    })\n",
    "            \n",
    "            all_results[model_name] = model_results\n",
    "    \n",
    "    print(f\"üìä Loaded {len(image_mapping)} images with {len(all_annotations)} ground truth captions\")\n",
    "    \n",
    "    # Create COCO ground truth object\n",
    "    coco_gt_data = {\n",
    "        'annotations': all_annotations,\n",
    "        'images': [{'id': img_id} for img_id in range(len(image_mapping))],\n",
    "        'info': {'description': 'Image Captioning Evaluation'},\n",
    "        'licenses': [],\n",
    "        'type': 'captions'\n",
    "    }\n",
    "    \n",
    "    # Initialize COCO ground truth\n",
    "    coco_gt = COCO()\n",
    "    coco_gt.dataset = coco_gt_data\n",
    "    coco_gt.createIndex()\n",
    "    \n",
    "    print(f\"‚úÖ Ground truth COCO object created\")\n",
    "    \n",
    "    # Evaluate each model\n",
    "    evaluation_results = {}\n",
    "    \n",
    "    for model_name, model_results in all_results.items():\n",
    "        print(f\"\\nüîç Evaluating {model_name.upper()}...\")\n",
    "        \n",
    "        if not model_results:\n",
    "            print(f\"‚ùå No valid predictions for {model_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Save results to temporary file (required by pycocoevalcap)\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as temp_file:\n",
    "            json.dump(model_results, temp_file)\n",
    "            temp_filename = temp_file.name\n",
    "        \n",
    "        try:\n",
    "            # Load results\n",
    "            coco_res = coco_gt.loadRes(temp_filename)\n",
    "            \n",
    "            # Create evaluator\n",
    "            coco_eval = COCOEvalCap(coco_gt, coco_res)\n",
    "            \n",
    "            # Run evaluation\n",
    "            coco_eval.evaluate()\n",
    "            \n",
    "            # Store results\n",
    "            evaluation_results[model_name] = coco_eval.eval.copy()\n",
    "            \n",
    "            print(f\"‚úÖ {model_name} evaluation complete\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error evaluating {model_name}: {e}\")\n",
    "            evaluation_results[model_name] = {}\n",
    "        \n",
    "        finally:\n",
    "            # Clean up temporary file\n",
    "            os.unlink(temp_filename)\n",
    "    \n",
    "    return evaluation_results, coco_gt\n",
    "\n",
    "def display_pycocoevalcap_results(evaluation_results):\n",
    "    \"\"\"Display results in a nice format\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üèÜ OFFICIAL COCO EVALUATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create comparison table\n",
    "    import pandas as pd\n",
    "    \n",
    "    table_data = []\n",
    "    for model_name, scores in evaluation_results.items():\n",
    "        if scores:  # Only include models with results\n",
    "            row = {\n",
    "                'Model': model_name.upper(),\n",
    "                'BLEU-1': f\"{scores.get('Bleu_1', 0):.4f}\",\n",
    "                'BLEU-2': f\"{scores.get('Bleu_2', 0):.4f}\",\n",
    "                'BLEU-3': f\"{scores.get('Bleu_3', 0):.4f}\",\n",
    "                'BLEU-4': f\"{scores.get('Bleu_4', 0):.4f}\",\n",
    "                'METEOR': f\"{scores.get('METEOR', 0):.4f}\",\n",
    "                'ROUGE-L': f\"{scores.get('ROUGE_L', 0):.4f}\",\n",
    "                'CIDEr': f\"{scores.get('CIDEr', 0):.4f}\",\n",
    "                'SPICE': f\"{scores.get('SPICE', 0):.4f}\"\n",
    "            }\n",
    "            table_data.append(row)\n",
    "    \n",
    "    if table_data:\n",
    "        df = pd.DataFrame(table_data)\n",
    "        print(df.to_string(index=False))\n",
    "        \n",
    "        # Find best performing models\n",
    "        print(f\"\\nBEST PERFORMING MODELS:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        metrics = ['Bleu_4', 'METEOR', 'ROUGE_L', 'CIDEr', 'SPICE']\n",
    "        metric_names = ['BLEU-4', 'METEOR', 'ROUGE-L', 'CIDEr', 'SPICE']\n",
    "        \n",
    "        for metric, name in zip(metrics, metric_names):\n",
    "            best_model = max(evaluation_results.keys(), \n",
    "                           key=lambda x: evaluation_results[x].get(metric, 0))\n",
    "            best_score = evaluation_results[best_model].get(metric, 0)\n",
    "            print(f\"{name:10}: {best_model.upper():10} ({best_score:.4f})\")\n",
    "\n",
    "    \n",
    "    return df if table_data else None\n",
    "\n",
    "# Main execution\n",
    "def run_official_evaluation():\n",
    "    \"\"\"Run the complete official evaluation\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Starting Official COCO Evaluation...\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    results, coco_gt = evaluate_with_pycocoevalcap(\"eval_json\")\n",
    "    \n",
    "    # Display results\n",
    "    summary_df = display_pycocoevalcap_results(results)\n",
    "    \n",
    "    # Save results\n",
    "    with open('official_coco_evaluation.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    if summary_df is not None:\n",
    "        summary_df.to_csv('official_evaluation_summary.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nüíæ Results saved to:\")\n",
    "    print(f\"  - official_coco_evaluation.json\")\n",
    "    print(f\"  - official_evaluation_summary.csv\")\n",
    "    \n",
    "    return results, summary_df\n",
    "\n",
    "# Run the evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6df1b298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Official COCO Evaluation...\n",
      "üöÄ Loading evaluation data...\n",
      "üìä Loaded 10 images with 50 ground truth captions\n",
      "creating index...\n",
      "index created!\n",
      "‚úÖ Ground truth COCO object created\n",
      "\n",
      "üîç Evaluating GIT...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 606 tokens at 17305.81 tokens per second.\n",
      "PTBTokenizer tokenized 200 tokens at 7120.86 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 176, 'reflen': 131, 'guess': [176, 166, 156, 146], 'correct': [110, 57, 24, 8]}\n",
      "ratio: 1.3435114503714236\n",
      "Bleu_1: 0.625\n",
      "Bleu_2: 0.463\n",
      "Bleu_3: 0.321\n",
      "Bleu_4: 0.206\n",
      "computing METEOR score...\n",
      "METEOR: 0.297\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.475\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.540\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.3 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.2 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.8 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Threads( StanfordCoreNLP ) [1.229 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 6.728 s\n",
      "SPICE: 0.228\n",
      "‚úÖ git evaluation complete\n",
      "\n",
      "üîç Evaluating BLIP2...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 114, 'reflen': 108, 'guess': [114, 104, 94, 84], 'correct': [57, 21, 2, 0]}\n",
      "ratio: 1.0555555555457818\n",
      "Bleu_1: 0.500\n",
      "Bleu_2: 0.318\n",
      "Bleu_3: 0.129\n",
      "Bleu_4: 0.000\n",
      "computing METEOR score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 606 tokens at 23283.11 tokens per second.\n",
      "PTBTokenizer tokenized 125 tokens at 4718.24 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR: 0.145\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.364\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.377\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.1 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.8 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [0.884 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 5.281 s\n",
      "SPICE: 0.100\n",
      "‚úÖ blip2 evaluation complete\n",
      "\n",
      "üîç Evaluating VIT_GPT2...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 257, 'reflen': 130, 'guess': [257, 247, 237, 227], 'correct': [91, 18, 3, 1]}\n",
      "ratio: 1.97692307690787\n",
      "Bleu_1: 0.354\n",
      "Bleu_2: 0.161\n",
      "Bleu_3: 0.069\n",
      "Bleu_4: 0.035\n",
      "computing METEOR score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 606 tokens at 23453.47 tokens per second.\n",
      "PTBTokenizer tokenized 282 tokens at 11247.01 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR: 0.188\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.313\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.121\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [1.0 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [3.381 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 8.198 s\n",
      "SPICE: 0.119\n",
      "‚úÖ vit_gpt2 evaluation complete\n",
      "\n",
      "üîç Evaluating BLIP...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 606 tokens at 19295.67 tokens per second.\n",
      "PTBTokenizer tokenized 249 tokens at 8149.45 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 236, 'reflen': 131, 'guess': [236, 226, 216, 206], 'correct': [126, 59, 24, 9]}\n",
      "ratio: 1.8015267175435\n",
      "Bleu_1: 0.534\n",
      "Bleu_2: 0.373\n",
      "Bleu_3: 0.249\n",
      "Bleu_4: 0.161\n",
      "computing METEOR score...\n",
      "METEOR: 0.283\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.401\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.152\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.9 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [3.108 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 7.754 s\n",
      "SPICE: 0.232\n",
      "‚úÖ blip evaluation complete\n",
      "\n",
      "================================================================================\n",
      "üèÜ OFFICIAL COCO EVALUATION RESULTS\n",
      "================================================================================\n",
      "   Model BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L  CIDEr  SPICE\n",
      "     GIT 0.6250 0.4633 0.3208 0.2062 0.2972  0.4749 0.5401 0.2283\n",
      "   BLIP2 0.5000 0.3177 0.1290 0.0000 0.1454  0.3642 0.3771 0.0999\n",
      "VIT_GPT2 0.3541 0.1606 0.0689 0.0346 0.1884  0.3127 0.1211 0.1192\n",
      "    BLIP 0.5339 0.3733 0.2493 0.1613 0.2827  0.4012 0.1519 0.2324\n",
      "\n",
      "ü•á BEST PERFORMING MODELS:\n",
      "--------------------------------------------------\n",
      "BLEU-4    : GIT        (0.2062)\n",
      "METEOR    : GIT        (0.2972)\n",
      "ROUGE-L   : GIT        (0.4749)\n",
      "CIDEr     : GIT        (0.5401)\n",
      "SPICE     : BLIP       (0.2324)\n",
      "\n",
      "üíæ Results saved to:\n",
      "  - official_coco_evaluation.json\n",
      "  - official_evaluation_summary.csv\n"
     ]
    }
   ],
   "source": [
    "evaluation_results, summary_df = run_official_evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
